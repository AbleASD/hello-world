<center><big><b><font size='8'>BpNeuralNets<font><b><big>
    
</center>

![神经网络](https://upload-images.jianshu.io/upload_images/9602672-aac10bca9990f5ed.png?imageMogr2/auto-orient/strip|imageView2/2/w/740/format/webp)



#### 数据来源：

​	[MNIST handwritten digit database, Yann LeCun, Corinna Cortes and Chris Burges](http://yann.lecun.com/exdb/mnist/)

​	**描述：0-9共十种手写图像(28\*28)数据集，其中训练集60000张，测试集10000张.**

#### 基本原理：

​	人工神经网络无需事先确定输入输出之间映射关系的数学方程，仅通过自身的训练，学习某种规则，在给定输入值时得到最接近期望输出值的结果。作为一种智能信息处理系统，人工神经网络实现其功能的核心是算法。BP神经网络是一种按**误差反向传播**(简称误差反传)训练的多层前馈网络，其算法称为BP算法，它的基本思想是梯度下降法，利用梯度搜索技术，以期使网络的实际输出值和期望输出值的误差均方差为最小。

​	基本BP算法包括信号的前向传播和误差的反向传播两个过程。即计算误差输出时按从输入到输出的方向进行，而调整权值和阈值则从输出到输入的方向进行。正向传播时，输入信号通过隐含层作用于输出节点，经过非线性变换，产生输出信号，若实际输出与期望输出不相符，则转入误差的反向传播过程。误差反传是将输出误差通过隐含层向输入层逐层反传，并将误差分摊给各层所有单元，以从各层获得的误差信号作为调整各单元权值的依据。通过调整输入节点与隐层节点的联接强度和隐层节点与输出节点的联接强度以及阈值，使误差沿梯度方向下降，经过反复学习训练，确定与最小误差相对应的网络参数(权值和阈值)，训练即告停止。此时经过训练的神经网络即能对类似样本的输入信息，自行处理输出误差最小的经过非线形转换的信息。

#### 优劣势：

​	BP神经网络无论在网络理论还是在性能方面已比较成熟。其突出优点就是具有很强的非线性映射能力和柔性的网络结构。网络的中间层数、各层的神经元个数可根据具体情况任意设定，并且随着结构的差异其性能也有所不同。但是BP神经网络也存在以下的一些主要缺陷。

①学习速度慢，即使是一个简单的问题，一般也需要几百次甚至上千次的学习才能收敛。

②容易陷入局部极小值。

③网络层数、神经元个数的选择没有相应的理论指导。

④网络推广能力有限。

对于上述问题，已经有了许多改进措施，研究最多的就是如何加速网络的收敛速度和尽量避免陷入局部极小值的问题。 



#### 算法实现：

 - 信号前向传播

   > 数据经由输入层，隐藏层，输出层输出模型的预测：
   > $$
   > z=wx+b
   > $$
   > ​		$z$表示预测值，其中$w$表示权重，$x$表示输入，$b$为偏置值，其中偏置值的存在将允许激活函数左或右移，这可能是成功学习的关键
   >
   > ​		为了使目标函数转换到非线性的问题上，使用$sigmod$,$tanh$,$ReLu$等函数将其转换到非线性的问题上，这样就可以处理非线性分类问题。即从输入层到隐藏层，通过如$a_1=sigmod(z)$转为为到下一层的输入
   >
   > ​		对于当前问题，由于是多分类问题，因此通过$softmax$将其转换层概率向量，我们的预测值$y\_pred$则将输入分配到输出概率最大的分类。
   >
   > 

 - 误差反向传播

   > 从输出层到隐含层再到输入层依次调整隐含层和输出层的权重和偏置：
   > $$
   > loss=\frac{\sum_{i=1}^{n}(y\_pred_i-y)^2}{2n}
   > $$
   > ​	通过loss函数描述训练得到的预测和真实值的偏差来反向调整参数$w$和偏置值$b$.
   >
   > ​	主要是通过链式求导法则来求解梯度并给定步长(学习率)来调整参数，使损失函数尽可能的小以达到提高预测精准度的目的。